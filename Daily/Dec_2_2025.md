# TIL: Scaling Data Mining for Real-World Logistics (ISYE 7406)

**Course:** ISYE 7406 â€“ Data Mining & Statistical Learning
**Project:** Grocery List Optimization Using Multi-Source Data Integration

This semester, my team tackled the challenge of **Food Insecurity** among college students. Our goal was to build a recommendation system that converts recipes into cost-optimized grocery lists by integrating USDA recipe datasets with Walmart product inventories.

While the project involved standard data mining techniques like Fuzzy Matching and Clustering (achieving a ~95.5% match rate), my most significant learning curve came from bridging the gap between **theoretical optimization** and **real-world feasibility**.

## ðŸš€ Key Technical Challenges & Contributions

### 1. From "Lowest Price" to "Landed Cost" (Spatial Constraints)
Our initial cost optimization algorithm was a greedy approach that simply minimized `Product_Price`. This led to unrealistic recommendationsâ€”such as the model suggesting a user in Atlanta buy a \$2 pack of salt from a store in Hawaii because it was 10 cents cheaper.

* **The Fix:** I introduced a **Spatial Constraint** by integrating ZIP code geodata. I mapped store identifiers to precise latitude/longitude coordinates and updated the objective function to minimize **Landed Cost** (`Price + Distance Ã— Shipping_Rate`).
* **The Consequence:** To allow the algorithm to find the "nearest" store, we could no longer deduplicate products by name. We had to retain inventory records for *every* store location. This caused our dataset to explode from **140,000 to over 2.6 million rows**.

### 2. Scaling GMM for Big Data
We utilized **Gaussian Mixture Models (GMM)** for soft clustering ingredients (e.g., distinguishing whether "cheese" is being used as a dairy staple or a topping). However, training the GMM on the expanded 2.6 million-row dataset became computationally prohibitive.

* **The Optimization:** I re-engineered the training pipeline using a **Sampling Strategy**. By training the model parameters on a statistically representative subset (~50k rows) and implementing **Batch Processing** for the prediction phase, I reduced the execution time from hours to minutes while maintaining an LDA validation accuracy of **~96%**.

### 3. Implementing "Basket Logic" (Economic Feasibility)
Initial test runs showed inflated total costs because the model was calculating shipping fees on an *item-wise* basis (e.g., charging shipping 5 times for 5 items from the same store).

* **The Logic:** I implemented a post-processing **"Basket Logic"** algorithm. This step groups selected items by store location and consolidates shipping fees (applying a single delivery charge per store). This prevented the model from fragmenting orders across distant retailers and significantly reduced the final cost presented to the user.

## ðŸ’¡ Conclusion & Reflection

As detailed in our final report, the project demonstrated that our data merging and clustering pipelines are highly effective, creating interpretable structure from noisy retail data. However, the addition of spatial and logistical constraints was the turning point that made the model viable.

**My biggest takeaway:**
Data Mining is not just about fitting models; it is about **context**. A mathematically optimal solution (lowest price) is useless if it is logistically impossible (wrong location). By solving the scalability issues of the 2.6M dataset and engineering constraints for reality, we transformed a theoretical class project into a robust tool that could genuinely assist students living on a budget.

***
