Here is the translated version of the document, maintaining your original formatting.

## W9_L1 + W9_L2
- ISyE7406 Lec.15 Unsupervised Learning - Cluster Analysis
    - Core Cluster Analysis Methods
        - Calculation Methods
            - Connectivity Models: Hierarchical Clustering
                - Result Format: Dendrogram
                - Types: Agglomerative (Bottom-up), Divisive (Top-down)
                - Linkage Criteria: Single Linkage, Complete Linkage, UPGMA
                - Disadvantages: Sensitive to outliers, high complexity ($O(n^3)/O(2^{n-1})$)
            - Centroid Models: K-Means
                - Objective: Minimize Sum of Within-Cluster Variances (SWCV)
                - Algorithm: Lloyd's Algorithm (approximate solution, local optimum)
                - Variants: k-medoids, k-medians, k-means++, fuzzy c-means
                - Disadvantages: Need to pre-specify k, prefers clusters of similar sizes
            - Density Models: DBSCAN/OPTICS
                - DBSCAN: Density reachability, linear complexity, stable results
                - OPTICS: No need to specify $\epsilon$, generates hierarchical results
            - Subspace Models: Biclustering
                - Logic: Cluster rows (samples) and columns (features) simultaneously
                - Types: Constant biclusters, constant row/column biclusters, coherent value biclusters
            - Neural Models: SOM/Multidimensional Scaling
                - SOM: Unsupervised ANN, dimensionality reduction, topology preservation (training + mapping modes)
                - Multidimensional Scaling: Low-dimensional visualization of high-dimensional data
            - Graph-based Models: Clique (mentioned but not expanded)
        - Statistical Methods: Multivariate Gaussian Mixture Models (GMM)
            - Features: Soft clustering, clusters can overlap, captures attribute correlation
            - Parameters: $\lambda$ (mixture weights), $\mu$ (mean vectors), $\Sigma$ (covariance matrices)
            - Estimation Algorithm: EM (E-Step calculates weights, M-Step updates parameters)
    - Efficient Calculation Algorithms for K-Means
        - Optimize Distance: Vectorization (NumPy), specialized metrics
        - Cluster Assignment: `np.argmin` to find nearest centroid
        - Centroid Update: Broadcasting mechanism to calculate means
        - Big Data Structures: KD-Trees/Ball Trees
        - Initialization: K-means++
        - Convergence: Tolerance threshold
        - Massive Data: Mini-Batch K-Means (incremental updates)
        - Parallelism: Multi-core, Apache Spark
            
    - Key Formulas and Concepts
        - SWCV Expression: $\sum_{i=1}^{K}\sum_{j=1}^{n_i}(y_{ij}-\bar{y}_i)^2$
        - TSSD Decomposition: TSSD = SWCV + SBCV (Sum of cross terms is 0)
            
    - Post-class Questions and Answers
        - K-Means: SWCV expression + optimization decision
        - GMM: Number of parameters when K=5, p=7 (139 parameters)
        - GMM: Purpose of E-Step (Calculate sample assignment probability weights)


## W10_L1
- ISyE7406 Lec.17 (10/21/25) Content Summary
    - I. Dimensionality Reduction Techniques
        - 1. PCA (Principal Component Analysis)
            - Core: Singular Value Decomposition (SVD) of X: $X_{n \times p} = U_{n \times p} D_{p \times p} V_{p \times p}^T$
                - U: Left singular vectors, $U^TU=I_p$
                - D: Diagonal matrix, elements are PC variances ($d_1 \ge d_2 \ge \dots \ge d_p$)
                - V: Orthogonal matrix, columns of VD are PCs
            - Key Properties: PCs are linear combinations of X, PCs are orthogonal, variances decrease
            - Derivative Applications: PCR (PCA Regression), Hat Matrix $H_k = P_k(T_k^TT_k)^{-1}T_k^T$
                
        - 2. PLS (Partial Least Squares Regression)
            - Concept: Project X and Y to new space, model covariance structure (distinct from PCA which only maximizes X variance)
            - Technical Details: Regression model $Y=X\beta+e$, Hat Matrix $H_{k,PLS} = W_k(P_k^TW_k)^{-1}(T_k^TT_k)^{-1}T_k^T$
            - Limitations: PLS variables contain all original variables, difficult calculation when $n \ll p$, weak physical interpretation
                
    - II. Variable Selection Methods
        - 1. Traditional Methods ($n>p$ scenario)
            - Stepwise Regression: Initially select X with highest correlation to Y $\rightarrow$ Add significant variables via F-test ($\alpha_{in}=0.15$) $\rightarrow$ Remove insignificant variables via F-test ($\alpha_{out}=0.15$)
            - Best Subset Selection: Based on 4 major criteria
                - Adjusted $R^2$ (Formula: $1-(1-R^2)((n-1)/(n-p-1))$, higher is better)
                - AIC ($2p-2\log(\text{Likelihood})$), BIC ($p\log(n)-2\log(\text{Likelihood})$, lower is better)
                - PRESS ($\sum(y_i-\hat{y}_{(i)})^2$, smaller is more robust)
                - Cp ($SSE(M_p)/\hat{\sigma}_k^2 -n+2p$, needs to be close to p)
        - 2. High-dimensional Methods ($n<p$ scenario)
            - Regularization Methods:
                - Ridge: Minimize $RSS+\lambda||\beta||_2^2$ (L2 penalty, no sparsity)
                - Lasso: Minimize $RSS+\lambda||\beta||_1$ (L1 penalty, sparsity)
                - Elastic Net: Minimize $||y-X\beta||^2+\lambda_1||\beta||_1+\lambda_2||\beta||_2^2$ (Combines advantages of both)
                - Adaptive Lasso: Weighted L1 penalty (weight is $1/|\hat{\beta}_{1st}|$, smaller bias)
            - CARS (Clustering and Representative Selection):
                - Optimization Objective: $1/2||Y-X\beta||_2^2+\lambda_1||\beta||_1+\lambda_2[\sum\sum d(j,m_k)+(1-\rho_{min})|C_0|]$
                - Constraints: $\le 1$ non-zero $\beta$ per cluster; representative of non-zero $\beta$ cluster is the medoid
                - Advantages: Combines Lasso sparsity with clustering, handles variable correlation
        - 3. Experimental Results
            - Simulation Experiments: CARS TP (93.5%-95.8%), FP (9.0%-9.3%) outperforms Glasso (TP $\approx$ 99%, FP $\approx$ 87%), CRL (same as Glasso)
            - Real Data (QEEG Depression Data):
                - Data: 15 patients, 598 QEEG signals, BDI-II scale
                - $R^2$ Comparison: CARS and Lasso both 86.9%, superior to Glasso (17.6%), CRL (30.5%)
                - SSE Improvement: CARS improved by 64.76% compared to Lasso
                - Bootstrap Validation: C6, C16, C18 are important clusters, F8TRP etc. are representative variables

## W10_L2, W11_L1 & L2
- Lec.18 Core Topics (ISyE7406 10/23/25)
    - I. Resampling Methods
        - Core Purpose
            - Estimate precision of sample statistics (median, variance, etc.)
            - Significance testing (Permutation test)
            - Model validation (e.g., Cross Validation)
        - Bootstrap
            - Operation: Sample $n$ items with replacement, generate $m$ (e.g., 10,000) bootstrap samples
            - Applications: Estimate bias ($Bias=\hat{\theta}_{orig} - \hat{\theta}_{ave\_boot}$), variance ($\hat{\sigma}_{b}^2$), MSE
        - Jackknife
            - Operation: Remove $1/d$ samples without replacement, generate $n$ samples (needs large $n$)
            - Advantages: Lower computational cost than Bootstrap
        - Cross Validation
            - Operation: Remove 1 sample to model $\rightarrow$ Predict removed sample $\rightarrow$ Calculate variance of $n$ prediction errors
            - Applications: Model selection (e.g., Kernel regression window $h$, Spline penalty $\lambda$)
    - II. Ensemble Methods
        - Bagging
            - Principle: Bootstrap generate B samples $\rightarrow$ Build B models $\rightarrow$ Average prediction (regression) / Majority vote (classification)
            - Formula: $\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}(x)$
        - Stacking
            - Principle: Multi-model Leave-One-Out prediction $\rightarrow$ Weighted combination (Minimize residual sum of squares to find weights $\hat{w}^{st}$)
            - Formula: Final prediction $\sum_{m}\hat{w}_m^{st}\hat{f}_m(x)$
        - Boosting (e.g., AdaBoost)
            - Core: Learn from errors (weighted misclassified samples), trust high-quality models ($\alpha_m=\log((1-err_m)/err_m)$)
            - Algorithm: Initialize weights $\rightarrow$ Iteratively build classifiers $\rightarrow$ Update weights $\rightarrow$ Combine classifiers ($G(x)=\text{sign}[\sum_{m=1}^{M}\alpha_m G_m(x)]$)
        - Random Forest
            - Algorithm: Bootstrap sampling $\rightarrow$ Randomly select $m$ variables at each node $\rightarrow$ Build B trees $\rightarrow$ Regression average / Classification vote
            - Prediction: Regression $\hat{f}_{rf}^B(x)=\frac{1}{B}\sum_{b=1}^{B}T_b(x)$; Classification takes majority vote
    - III. FDA and Extensions (CP-3 Topic)
        - Background: Solve LDA's lack of linear boundaries, high variance with many parameters, single prototype limitation
        - FDA (Flexible Discriminant Analysis)
            - Principle: Convert LDA to regression problem, extend with non-parametric regression (e.g., BRUTO, MARS)
            - Formula: $ASR=\frac{1}{N}\sum_{\ell=1}^{L}[\sum_{i=1}^{N}(\theta_{\ell}(g_i)-\eta_{\ell}(x_i))^2+\lambda J(\eta_{\ell})]$
            - Real Estate Data Optimum: MARS method + 3rd degree polynomial (Test Accuracy 80.77%)
        - PDA (Penalized Discriminant Analysis)
            - Principle: Add penalty term to FDA ($\lambda\beta_{\ell}^T\Omega\beta_{\ell}$), smoothing coefficient
            - Formula: Penalized Mahalanobis Distance $D(x,\mu)=(h(x)-h(\mu))^T(\Sigma_W+\lambda\Omega)^{-1}(h(x)-h(\mu))$
            - Real Estate Data Optimum: 4 degrees of freedom (Test Accuracy 79.81%)
        - MDA (Mixture Discriminant Analysis)
            - Principle: Model each class with $R_k$ mixture Gaussian prototypes, estimate parameters via E-M algorithm
            - Formula: Class conditional density $P(X|G=k)=\sum_{r=1}^{R_k}\pi_{kr}\phi(X;\mu_{kr},\Sigma)$
            - Real Estate Data Optimum: 6 subclasses (Test Accuracy 85.58%)
    - IV. Case Data (Taiwan Real Estate Valuation)
        - Variables: X1 (Transaction Date) - X6 (Lat/Long), Y (Price/Unit Area, 10,000 NTD/Ping)
        - Preprocessing: $Y \ge 37$ is "High" (1), $<37$ is "Low" (0); 75% Training / 25% Testing
        - Model Comparison: MDA (6 subclasses) > FDA (MARS3) > Logistic Regression > LDA/QDA/SVM

## W12
- ISyE7406 Lec.21 (11/04/24) Core Content
    - Generalized Additive Models (GAM)
        - Model Definition: $Y=\alpha+\sum_{j=1}^{p} f_{j}(X_{j})+\varepsilon$, PRSS criterion (penalized sum of squares)
        - BRUTO: Adaptive Backfitting Algorithm
            - Initialization: $\hat{\alpha}=ave(y_i)$, $\hat{f}_j \equiv 0$
            - Iteration: For each $j$, update $\hat{f}_j$ by smoothing residuals
        - MARS (Multivariate Adaptive Regression Splines)
            - Basis Functions: $(X_j-t)_{+}$, $(t-X_j)_{+}$ (Linear splines, knots at observations)
            - Modeling Steps: Forward selection (add basis function products to reduce residuals) $\rightarrow$ Backward deletion (remove term with minimal residual increase)
            - Model Selection: GCV criterion (Formula 9.20: $GCV(\lambda )=\frac{\sum _{i=1}^{N}(y_{i}-\hat{f}_{\lambda }(x_{i}))^{2}}{(1-M(\lambda)/ N)^{2}}$)
    - FDA Model Fitting Example (Bruto & MARS)
        - Data: Vowel recognition data
        - Performance Comparison: Table showing training/testing errors for LDA, CART, FDA/BRUTO, FDA/MARS, etc.
        - FDA Principle Review: Replace linear regression with non-parametric regression (e.g., GAM, MARS), optimize class scores $\theta_{\ell}(g)$ and mapping $\eta_{\ell}(X)$
    - Wavelet Analysis
        - Basic Definitions: Wavelet $\psi(t) \in L^2(\mathbb{R})$ (zero mean, unit norm), Scaling function $\phi(t)$
        - DWT (Discrete Wavelet Transform)
            - Coefficients: $c_{L,k}$ (coarse resolution, smooth), $d_{j,k}$ (fine resolution, detail)
            - Inverse Transform: $\hat{x}=H^T y$ (e.g., Haar wavelet example)
        - MRA (Multi-Resolution Analysis)
            - Resolution Levels: Coarse (e.g., L=0, 2 coefficients) $\rightarrow$ Fine (e.g., d1, 512 coefficients, when N=1024)
            - Example: AAPL stock price (1024 days) MRA coefficient distribution
        - Core Applications
            - Data Compression: AMDL criterion (Approximate Minimum Description Length)
            - Data Denoising: Shrinkage method (VisuShrink, RiskShrink, Threshold $\tau$)
            - Data Dimensionality Reduction: $RRE_h$ criterion (Balance energy loss and number of coefficients)
        - R Package Tools: `wavelets` (dwt/idwt functions), `waveslim` (1D/2D wavelet analysis)

## W9_L1 + W9_L2
- ISyE7406 Lec.15 无监督学习-聚类分析
    - 聚类分析核心方法
        - 计算方法
            - 连接模型：层次聚类
                - 结果形式：树状图（dendrogram）
                - 类型：凝聚型（自下而上）、分裂型（自上而下）
                - Linkage准则：单链接、全链接、UPGMA
                - 缺点：对异常值敏感、复杂度高（O(n³)/O(2ⁿ⁻¹)）
            - 质心模型：K-Means
                - 目标：最小化簇内平方和（SWCV）
                - 算法：Lloyd算法（近似解，局部最优）
                - 变体：k-medoids、k-medians、k-means++、模糊c-means
                - 缺点：需预指定k、偏好相似大小簇
            - 密度模型：DBSCAN/OPTICS
                - DBSCAN：密度可达性、线性复杂度、稳定结果
                - OPTICS：无需指定ε、生成层级结果
            - 子空间模型：双聚类
                - 逻辑：同时聚类行（样本）和列（特征）
                - 类型：常数双簇、行/列常数双簇、相干值双簇
            - 神经模型：SOM/多维缩放
                - SOM：无监督ANN、降维、拓扑保留（训练+映射模式）
                - 多维缩放：高维数据低维可视化
            - 图基模型：Clique（提及未展开）
        - 统计方法：多元正态混合模型（GMM）
            - 特性：软聚类、簇可重叠、捕获属性相关性
            - 参数：λ（混合权重）、μ（均值向量）、Σ（协方差矩阵）
            - 估计算法：EM（E-Step算权重、M-Step更参数）
    - K-Means高效计算算法
        - 优化距离：向量化（NumPy）、专用度量
        - 簇分配：np.argmin找最近质心
        - 质心更新：广播机制算均值
        - 大数据结构：KD-Trees/Ball Trees
        - 初始化：K-means++
        - 收敛：容忍度阈值
        - 超大数据：Mini-Batch K-Means（增量更新）
        - 并行：多核心、Apache Spark
            
    - 关键公式与概念
        - SWCV表达式：∑(i=1到K)∑(j=1到n_i)(y_ij-ȳ_i)²
        - TSSD分解：TSSD=SWCV+SBCV（交叉项和为0）
            
    - 课后问题与答案
        - K-Means：SWCV表达式+优化决策
        - GMM：K=5、p=7时参数数量（139个）
        - GMM：E-Step目的（算样本归属概率权重）


## W10_L1
- ISyE7406 Lec.17（10/21/25）内容总结
    - 一、降维技术
        - 1. PCA（主成分分析）
            - 核心：X的奇异值分解（SVD）：Xₙₓₚ = Uₙₓₚ Dₚₚ Vₚₚᵀ
                - U：左奇异向量，UᵀU=Iₚ
                - D：对角矩阵，元素为PC方差（d₁≥d₂≥…≥dₚ）
                - V：正交矩阵，VD列向量为PC
            - 关键性质：PC为X线性组合、PC间正交、方差递减
            - 衍生应用：PCR（PCA回归），头矩阵Hₖ = Pₖ(TₖᵀTₖ)⁻¹Tₖᵀ
                
        - 2. PLS（偏最小二乘回归）
            - 概念：投影X与Y至新空间，建模协方差结构（区别于PCA仅最大化X方差）
            - 技术细节：回归模型Y=Xβ+e，头矩阵Hₖ,PLS = Wₖ(PₖᵀWₖ)⁻¹(TₖᵀTₖ)⁻¹Tₖᵀ
            - 局限：PLS变量含所有原始变量，n<<p时计算困难，物理意义弱
                
    - 二、变量选择方法
        - 1. 传统方法（n>p场景）
            - 逐步回归：初始选与Y相关性最高X→添加F检验显著变量（α₍进₎=0.15）→移除F检验不显著变量（α₍出₎=0.15）
            - 最佳子集选择：基于4大准则
                - Adjusted R²（公式：1-(1-R²)((n-1)/(n-p-1))，越高越好）
                - AIC（2p-2log(Likelihood)）、BIC（plog(n)-2log(Likelihood)，越小越好）
                - PRESS（∑(yᵢ-ŷ₍ᵢ₎)²，越小越稳健）
                - Cp（SSE(Mₚ)/σ̂ₖ² -n+2p，需接近p）
        - 2. 高维方法（n<p场景）
            - 正则化方法：
                - Ridge：最小化RSS+λ||β||₂²（L2 penalty，无稀疏性）
                - Lasso：最小化RSS+λ||β||₁（L1 penalty，稀疏性）
                - Elastic Net：最小化||y-Xβ||²+λ₁||β||₁+λ₂||β||₂²（结合两者优势）
                - Adaptive Lasso：加权L1 penalty（权重为1/|β̂₍₁ₛₜ₎|， bias更小）
            - CARS（聚类与代表选择）：
                - 优化目标：1/2||Y-Xβ||₂²+λ₁||β||₁+λ₂[∑∑d(j,mₖ)+(1-ρₘᵢₙ)|C₀|]
                - 约束：每集群≤1个非零β；非零β集群的代表为medoid
                - 优势：结合Lasso稀疏性与聚类，处理变量相关性
        - 3. 实验结果
            - 模拟实验：CARS的TP（93.5%-95.8%）、FP（9.0%-9.3%）优于Glasso（TP≈99%、FP≈87%）、CRL（同Glasso）
            - 真实数据（QEEG抑郁数据）：
                - 数据：15患者，598个QEEG信号，BDI-II量表
                - R²对比：CARS与Lasso均86.9%，优于Glasso（17.6%）、CRL（30.5%）
                - SSE改进：CARS较Lasso提升64.76%
                - Bootstrap验证：C6、C16、C18为重要集群，F8TRP等为代表变量
## W10_L2、W11_L1 & L2
- Lec.18 核心主题（ISyE7406 10/23/25）
    - 一、重采样方法
        - 核心目的
            - 估计样本统计量（中位数、方差等）精度
            - 显著性检验（置换检验）
            - 模型验证（如交叉验证）
        - Bootstrap
            - 操作：有放回采样n个样本，生成m（如10,000）个bootstrap样本
            - 应用：估计偏差（Bias=θ_hat_orig - θ_hat_ave_boot）、方差（σ_b²_hat）、MSE
        - Jackknife
            - 操作：无放回删1/d个样本，生成n个样本（需大n）
            - 优势：计算量低于Bootstrap
        - 交叉验证
            - 操作：删1个样本建模→预测删除样本→计算n个预测误差的方差
            - 应用：模型选择（如Kernel回归窗口h、Spline惩罚λ）
    - 二、集成方法
        - Bagging
            - 原理：bootstrap生成B个样本→建B个模型→平均预测（回归）/多数投票（分类）
            - 公式：f^​bag​(x)=B1​∑b=1B​f^​∗b(x)
        - Stacking
            - 原理：多模型留一法预测→加权组合（最小化残差平方和求权重w^st）
            - 公式：最终预测∑m​w^mst​f^​m​(x)
        - Boosting（以AdaBoost为例）
            - 核心：从错误中学习（加权错分样本）、信任优质模型（α_m=log((1-err_m)/err_m)）
            - 算法：初始化权重→迭代建分类器→更新权重→组合分类器（G(x)=sign[∑m=1M​αm​Gm​(x)]）
        - Random Forest
            - 算法：bootstrap采样→每节点随机选m个变量→建B棵树→回归平均/分类投票
            - 预测：回归f^​rfB​(x)=B1​∑b=1B​Tb​(x)；分类取多数票
    - 三、FDA及扩展（CP-3主题）
        - 背景：解决LDA线性边界不足、参数多方差高、单原型局限
        - FDA（灵活判别分析）
            - 原理：将LDA转化为回归问题，用非参数回归（如BRUTO、MARS）扩展
            - 公式：ASR=N1​∑ℓ=1L​[∑i=1N​(θℓ​(gi​)−ηℓ​(xi​))2+λJ(ηℓ​)]
            - 房产数据最优：MARS方法+3次多项式（测试准确率80.77%）
        - PDA（惩罚判别分析）
            - 原理：FDA基础上加惩罚项（λβℓT​Ωβℓ​），平滑系数
            - 公式：惩罚马氏距离D(x,μ)=(h(x)−h(μ))T(∑W​+λΩ)−1(h(x)−h(μ))
            - 房产数据最优：4自由度（测试准确率79.81%）
        - MDA（混合判别分析）
            - 原理：每类用R_k个混合高斯原型建模，E-M算法估计参数
            - 公式：类条件密度P(X∣G=k)=∑r=1Rk​​πkr​ϕ(X;μkr​,∑)
            - 房产数据最优：6个子类（测试准确率85.58%）
    - 四、案例数据（台湾房产估值）
        - 变量：X1（交易日期）-X6（经纬度），Y（房价/单位面积，10,000新台币/坪）
        - 预处理：Y≥37为“高”（1），<37为“低”（0）；75%训练/25%测试
        - 模型对比：MDA（6子类）>FDA（MARS3）>逻辑回归> LDA/QDA/SVM
## W12
- ISyE7406 Lec.21（11/04/24）核心内容
    - 广义加性模型（GAM）
        - 模型定义：(Y=\alpha+\sum_{j=1}^{p} f_{j}(X_{j})+\varepsilon)，PRSS准则（ penalized sum of squares）
        - BRUTO：自适应Backfitting算法
            - 初始化：(\hat{\alpha}=ave(y_i))，(\hat{f}_j \equiv 0)
            - 迭代：对每个j，平滑残差更新(\hat{f}_j)
        - MARS（多元自适应回归样条）
            - 基函数：((X_j-t)_{+})、((t-X_j)_{+})（线性样条，结在观测值处）
            - 建模步骤：前向选择（添加基函数乘积减小残差）→ 后向删除（删除残差增最小项）
            - 模型选择：GCV准则（公式9.20：(GCV(\lambda )=\frac{\sum _{i=1}^{N}(y_{i}-\hat{f}_{\lambda }(x_{i}))^{2}}{(1-M(\lambda)/ N)^{2}})）
    - FDA模型拟合示例（Bruto & MARS）
        - 数据：元音识别数据
        - 性能对比：表格展示LDA、CART、FDA/BRUTO、FDA/MARS等技术的训练/测试误差
        - FDA原理回顾：通过非参数回归（如GAM、MARS）替代线性回归，优化类得分(\theta_{\ell}(g))与映射(\eta_{\ell}(X))
    - 小波分析
        - 基础定义：小波(\psi(t) \in L^2(\mathbb{R}))（零均值、单位范数），尺度函数(\phi(t))
        - DWT（离散小波变换）
            - 系数：(c_{L,k})（粗分辨率，平滑）、(d_{j,k})（细分辨率，细节）
            - 逆变换：(\hat{x}=H^T y)（如Haar小波示例）
        - MRA（多分辨率分析）
            - 分辨率级别：粗（如L=0，2个系数）→ 细（如d1，512个系数，N=1024时）
            - 示例：AAPL股价（1024天）的MRA系数分布
        - 核心应用
            - 数据压缩：AMDL准则（最小化描述长度）
            - 数据去噪：Shrinkage法（VisuShrink、RiskShrink，阈值(\tau)）
            - 数据降维：(RRE_h)准则（平衡能量损失与系数数量）
        - R包工具：`wavelets`（dwt/idwt函数）、`waveslim`（1D/2D小波分析）
